{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path, label):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['label'] = label\n",
    "    df['requests_rate'] = 1\n",
    "    return df\n",
    "\n",
    "def process_raw_data(dataset):\n",
    "    dataset['calculated_bwd_avg_segment_size'] = (\n",
    "        dataset['bwd_total_payload_bytes'] / (dataset['bwd_packets_count'] + 1)\n",
    "    )\n",
    "\n",
    "def post_process_aggregations(dataset):\n",
    "    dataset['requests_rate_src_port'] = dataset['requests_rate'] * dataset['src_ip']\n",
    "    dataset['requests_rate_dst_port'] = dataset['requests_rate'] * dataset['dst_ip']\n",
    "    return ['requests_rate_src_port', 'requests_rate_dst_port']\n",
    "\n",
    "def prepare_aggregated_dataset(df, aggregations, frequency='1s', include_labels=False):\n",
    "    if include_labels:\n",
    "        aggregations['label'] = lambda x: 1 if x.eq('benign').all() else -1\n",
    "\n",
    "    df_agg = df.resample(frequency).agg(aggregations)\n",
    "\n",
    "    if include_labels:\n",
    "        aggregations.pop('label', None)\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "def engineer_portscan_features(df):\n",
    "    if 'dst_port' in df.columns and 'requests_rate' in df.columns:\n",
    "        df['unique_dst_port_ratio'] = df['dst_port'] / (df['requests_rate'] + 1)\n",
    "    if 'src_port' in df.columns and 'requests_rate' in df.columns:\n",
    "        df['unique_src_port_ratio'] = df['src_port'] / (df['requests_rate'] + 1)\n",
    "    return df\n",
    "\n",
    "def engineer_extra_features(df):\n",
    "    if 'fwd_packets_count' in df.columns and 'packets_count' in df.columns:\n",
    "        df['fwd_packet_fraction'] = df['fwd_packets_count'] / (df['packets_count'] + 1)\n",
    "    else:\n",
    "        df['fwd_packet_fraction'] = 0\n",
    "    \n",
    "    if 'syn_flag_counts' in df.columns and 'packets_count' in df.columns:\n",
    "        df['frac_syn'] = df['syn_flag_counts'] / (df['packets_count'] + 1)\n",
    "    else:\n",
    "        df['frac_syn'] = 0\n",
    "    \n",
    "    if 'rst_flag_counts' in df.columns and 'packets_count' in df.columns:\n",
    "        df['frac_rst'] = df['rst_flag_counts'] / (df['packets_count'] + 1)\n",
    "    else:\n",
    "        df['frac_rst'] = 0\n",
    "    \n",
    "    if 'total_header_bytes' in df.columns and 'total_payload_bytes' in df.columns:\n",
    "        df['header_to_payload_ratio'] = (\n",
    "            df['total_header_bytes'] / (df['total_payload_bytes'] + 1)\n",
    "        )\n",
    "    else:\n",
    "        df['header_to_payload_ratio'] = 0\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_rolling_features(df, window='5s'):\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    if 'syn_flag_counts' in df.columns:\n",
    "        df['rolling_syn_5s'] = (\n",
    "            df['syn_flag_counts']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .sum()\n",
    "        )\n",
    "    else:\n",
    "        df['rolling_syn_5s'] = 0\n",
    "\n",
    "    if 'packets_count' in df.columns:\n",
    "        df['rolling_packets_mean_5s'] = (\n",
    "            df['packets_count']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .mean()\n",
    "        )\n",
    "    else:\n",
    "        df['rolling_packets_mean_5s'] = 0\n",
    "\n",
    "    if 'bytes_rate' in df.columns:\n",
    "        df['rolling_bytes_rate_std_5s'] = (\n",
    "            df['bytes_rate']\n",
    "            .rolling(window=window, min_periods=1)\n",
    "            .std()\n",
    "        )\n",
    "    else:\n",
    "        df['rolling_bytes_rate_std_5s'] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "def transform_aggregated_dataset(df, window='5s'):\n",
    "    post_process_aggregations(df)\n",
    "    engineer_portscan_features(df)\n",
    "\n",
    "    for col in ['src_port', 'dst_port']:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    engineer_extra_features(df)\n",
    "    engineer_rolling_features(df, window=window)\n",
    "\n",
    "    return df\n",
    "\n",
    "def prepare_dataset(\n",
    "    df,\n",
    "    aggregations,\n",
    "    frequency='1s',\n",
    "    include_labels=False,\n",
    "    window='5s',\n",
    "    filter_subnet=False\n",
    "): \n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], format='mixed', errors='coerce')\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    if filter_subnet and 'dst_ip' in df.columns:\n",
    "        df = df[df['dst_ip'].str.startswith('192.168', na=False)]\n",
    "\n",
    "    if 'protocol' in df.columns:\n",
    "        df = pd.get_dummies(df, columns=['protocol'], drop_first=True, dtype=int)\n",
    "        # Ensure aggregator counts these new protocol columns\n",
    "        for col in df.columns:\n",
    "            if col.startswith('protocol_') and (col not in aggregations):\n",
    "                aggregations[col] = 'count'\n",
    "\n",
    "    process_raw_data(df)\n",
    "\n",
    "    df_agg = prepare_aggregated_dataset(\n",
    "        df,\n",
    "        aggregations,\n",
    "        frequency=frequency,\n",
    "        include_labels=include_labels\n",
    "    )\n",
    "\n",
    "    transform_aggregated_dataset(df_agg, window=window)\n",
    "\n",
    "    features = df_agg.columns\n",
    "    exclude_cols = ['dst_ip', 'src_ip', 'dst_port', 'src_port']\n",
    "    features = [col for col in features if col not in exclude_cols]\n",
    "\n",
    "    return df_agg, features\n",
    "\n",
    "def safe_agg(func, default=0):\n",
    "    def wrapper(series):\n",
    "        if not series.empty:\n",
    "            return func(series)\n",
    "        else:\n",
    "            return default\n",
    "    return wrapper\n",
    "\n",
    "def combine_datasets(s1, s2):\n",
    "    combined_data = pd.concat([s1, s2])\n",
    "    combined_data.sort_index(inplace=True)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregations = {\n",
    "    'syn_flag_counts':          safe_agg(pd.Series.sum), \n",
    "    'rst_flag_counts':          safe_agg(pd.Series.sum),\n",
    "    'ack_flag_counts':          safe_agg(pd.Series.sum), \n",
    "    'packets_count':            safe_agg(pd.Series.sum),\n",
    "    'fwd_packets_count':        safe_agg(pd.Series.sum),\n",
    "    'bwd_packets_count':        safe_agg(pd.Series.sum),\n",
    "    'dst_port':                 safe_agg(pd.Series.nunique), \n",
    "    'src_port':                 safe_agg(pd.Series.nunique),\n",
    "    'src_ip':                   safe_agg(pd.Series.nunique),\n",
    "    'dst_ip':                   safe_agg(pd.Series.nunique),\n",
    "\n",
    "    'bytes_rate': safe_agg(pd.Series.mean), \n",
    "    'requests_rate': safe_agg(pd.Series.count),\n",
    "    'psh_flag_counts': safe_agg(pd.Series.sum),\n",
    "\n",
    "    'calculated_bwd_avg_segment_size': safe_agg(pd.Series.mean),\n",
    "    'bwd_payload_bytes_mean':          safe_agg(pd.Series.sum),\n",
    "    'bwd_init_win_bytes':             safe_agg(pd.Series.mean),\n",
    "    'subflow_bwd_bytes':              safe_agg(pd.Series.sum),\n",
    "    'bwd_total_payload_bytes':        safe_agg(pd.Series.sum),\n",
    "    'fwd_payload_bytes_min':          safe_agg(pd.Series.min),\n",
    "    'bwd_payload_bytes_max':          safe_agg(pd.Series.max),\n",
    "    \n",
    "    'packet_IAT_std':  safe_agg(pd.Series.std),\n",
    "    'packet_IAT_min':  safe_agg(pd.Series.min),\n",
    "    'packet_IAT_total': safe_agg(pd.Series.sum),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddos =  load_data(f\"./BCCC-CIC-IDS-2017/ddos_loit.csv\", \"malign\")\n",
    "benign = load_data(f\"./BCCC-CIC-IDS-2017/friday_benign.csv\", \"benign\")\n",
    "\n",
    "combined = combine_datasets(ddos, benign)\n",
    "\n",
    "combined, features = prepare_dataset(combined, aggregations, include_labels=True, filter_subnet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       1.00      1.00      1.00     27889\n",
      "     Anomaly       0.88      0.94      0.91      1083\n",
      "\n",
      "    accuracy                           0.99     28972\n",
      "   macro avg       0.94      0.97      0.95     28972\n",
      "weighted avg       0.99      0.99      0.99     28972\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def score_model(anomaly_score, y):\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y, anomaly_score, labels=[1, -1], target_names=[\"Normal\", \"Anomaly\"]))\n",
    "\n",
    "def find_best_contamination_for_isolation_forest(X, y):\n",
    "    best_f1, best_c = 0, None\n",
    "    for c in np.arange(0.01, 0.2, 0.01):\n",
    "        model = IsolationForest(contamination=c, random_state=42)\n",
    "        anomaly_score = model.fit_predict(X)\n",
    "\n",
    "        _, _, f1, _ = precision_recall_fscore_support(y, anomaly_score, average='binary')\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_c = c\n",
    "    \n",
    "    return best_c, best_f1\n",
    "\n",
    "X = combined[features]\n",
    "y = combined['label']\n",
    "\n",
    "c, f1 = find_best_contamination_for_isolation_forest(X, y)\n",
    "model = IsolationForest(contamination=c, random_state=42)\n",
    "anomaly_score = model.fit_predict(X)\n",
    "score_model(anomaly_score, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
